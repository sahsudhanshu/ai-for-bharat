"""
LangGraph graph definition â€” the core agent orchestration.

Graph flow:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ language_guardâ”‚â”€â”€rejectedâ”€â”€â–º END (rejection message)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ accepted
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ load_context  â”‚  (memory + region + catch context)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    agent      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
         â”‚                   â”‚
    has_tool_calls?          â”‚
      yes â”‚    no            â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”‚
  â”‚ tool_executor â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (no more tool calls)
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ memory_update â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
        END
"""
from __future__ import annotations
from typing import Any, Dict, Literal

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langgraph.graph import StateGraph, END

from src.core.state import AgentState
from src.core.prompts import build_system_prompt
from src.memory.manager import build_message_history, extract_and_update_long_term_memory
from src.memory.dynamodb_store import get_long_term_memory
from src.utils.languages import validate_language, get_rejection_message
from src.tools.weather import get_weather
from src.tools.catch_history import get_catch_history
from src.tools.specific_catch import get_catch_details
from src.tools.map_data import get_map_data
from src.tools.market_prices import get_market_prices


# â”€â”€ All tools the agent can invoke â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOOLS = [get_weather, get_catch_history, get_catch_details, get_map_data, get_market_prices]

# â”€â”€ LLM with tools bound â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_llm():
    import os
    api_key = os.getenv("GOOGLE_API_KEY", "")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY environment variable not set")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        google_api_key=api_key,
        max_output_tokens=2048,
        temperature=0.7,
    )
    return llm.bind_tools(TOOLS)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Node: language_guard
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def language_guard(state: AgentState) -> Dict[str, Any]:
    """Validate that the user's input matches the selected language."""
    text = state["human_input"]
    lang = state.get("selected_language", "en")
    accepted, reason = validate_language(text, lang)

    if not accepted:
        rejection = get_rejection_message(lang)
        if reason:
            rejection = f"{reason}\n\n{rejection}"
        return {
            "language_accepted": False,
            "language_rejection": rejection,
        }

    return {"language_accepted": True, "language_rejection": None}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Node: load_context
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def load_context(state: AgentState) -> Dict[str, Any]:
    """Load memory, summary, and any relevant context into state."""
    conversation_id = state["conversation_id"]
    user_id = state["user_id"]
    lang = state.get("selected_language", "en")

    # Build message history (last N verbatim + summary of older)
    recent_messages, summary = await build_message_history(conversation_id)

    # Long-term memory
    ltm = get_long_term_memory(user_id)

    # Build system prompt with all context
    system_prompt = build_system_prompt(
        selected_language=lang,
        summary=summary,
        long_term_memory=ltm,
    )

    # Compose the full messages list
    messages = [SystemMessage(content=system_prompt)]
    messages.extend(recent_messages)
    messages.append(HumanMessage(content=state["human_input"]))

    return {
        "messages": messages,
        "summary": summary,
        "long_term_memory": ltm,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Mock LLM fallback (when Bedrock is unavailable)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_MOCK_RESPONSES_BY_LANG = {
    "en": [
        "Based on current sea conditions near the Konkan coast, today is a good day for fishing! Wind speed is moderate at 3-4 m/s from the northwest. I recommend heading out early morning between 0400-0900 IST for the best catch. Indian Pomfret and Mackerel are in season. ðŸŸ",
        "Namaste! The weather looks favorable for the next 3 days. Sea surface temperature is around 28Â°C which is ideal for Tuna and Seer Fish. However, please avoid venturing beyond 12 nautical miles as there are reports of rough patches further out. Stay safe! ðŸŒŠ",
        "Great question! Based on recent market data, Pomfret is fetching â‚¹750-800/kg at Mumbai's Sassoon Docks. Surmai (Seer Fish) is at â‚¹700/kg with high demand. I'd suggest selling your Pomfret catch today while prices are up. For Mackerel, prices are stable at â‚¹200/kg. ðŸ’°",
        "The fishing ban period along the west coast (June 1 - July 31) doesn't apply to traditional non-mechanised boats. If you're using a motorised trawler, please ensure your license is current. The PM Matsya Sampada Yojana offers subsidies up to â‚¹3 lakh for equipment upgrades. Visit your district fisheries office for more details. ðŸ“‹",
        "For the best catch quality, remember to ice your fish immediately after catching. Maintain a temperature of 0-4Â°C. Gut larger fish within 2 hours. Premium grade fish can earn you â‚¹120-200/kg more than Standard grade â€” that's a big difference over a season! ðŸ§Š",
    ],
    "ta": [
        "à®•à¯Šà®™à¯à®•à®©à¯ à®•à®Ÿà®±à¯à®•à®°à¯ˆà®•à¯à®•à¯ à®…à®°à¯à®•à®¿à®²à¯à®³à¯à®³ à®¤à®±à¯à®ªà¯‹à®¤à¯ˆà®¯ à®•à®Ÿà®²à¯ à®¨à®¿à®²à¯ˆà®®à¯ˆà®•à®³à®¿à®©à¯ à®…à®Ÿà®¿à®ªà¯à®ªà®Ÿà¯ˆà®¯à®¿à®²à¯, à®‡à®©à¯à®±à¯ à®®à¯€à®©à¯à®ªà®¿à®Ÿà®¿à®•à¯à®• à®’à®°à¯ à®¨à®²à¯à®² à®¨à®¾à®³à¯! à®•à®¾à®±à¯à®±à®¾à®²à¯ˆ à®®à¯‡à®±à¯à®•à¯ à®¤à®¿à®šà¯ˆà®¯à®¿à®²à®¿à®°à¯à®¨à¯à®¤à¯ 3-4 à®®à¯€/à®µà®¿ à®µà¯‡à®•à®¤à¯à®¤à®¿à®²à¯ à®®à®¿à®¤à®®à®¾à®• à®‰à®³à¯à®³à®¤à¯. à®šà®¿à®±à®¨à¯à®¤ à®ªà®¿à®Ÿà®¿à®ªà¯à®ªà®¿à®±à¯à®•à®¾à®• à®•à®¾à®²à¯ˆ 0400-0900 IST à®•à¯à®•à¯ à®‡à®Ÿà¯ˆà®¯à®¿à®²à¯ à®šà¯†à®²à¯à®µà®¤à®±à¯à®•à¯ à®ªà®°à®¿à®¨à¯à®¤à¯à®°à¯ˆà®•à¯à®•à®¿à®±à¯‡à®©à¯. à®ªà®¾à®®à¯à®ƒà®ªà¯à®°à¯†à®Ÿà¯ à®®à®±à¯à®±à¯à®®à¯ à®•à®¾à®©à®¾à®™à¯à®•à¯†à®³à¯à®¤à¯à®¤à®¿ à®ªà®°à¯à®µà®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³à®©. ðŸŸ",
        "à®¨à®®à®¸à¯à®•à®¾à®°à®®à¯! à®…à®Ÿà¯à®¤à¯à®¤ 3 à®¨à®¾à®Ÿà¯à®•à®³à¯à®•à¯à®•à¯ à®µà®¾à®©à®¿à®²à¯ˆ à®šà®¾à®¤à®•à®®à®¾à®• à®¤à¯†à®°à®¿à®•à®¿à®±à®¤à¯. à®•à®Ÿà®²à¯ à®ªà®°à®ªà¯à®ªà®³à®µà¯ à®šà¯à®®à®¾à®°à¯ 28Â°C à®µà¯†à®ªà¯à®ªà®¨à®¿à®²à¯ˆà®¯à®¿à®²à¯ à®‰à®³à¯à®³à®¤à¯, à®‡à®¤à¯ à®šà¯‚à®°à¯ˆ à®®à®±à¯à®±à¯à®®à¯ à®šà¯€à®²à®¾ à®®à¯€à®©à¯à®•à®³à¯à®•à¯à®•à¯ à®à®±à¯à®±à®¤à¯. à®‡à®°à¯à®¨à¯à®¤à®¾à®²à¯à®®à¯, à®•à®Ÿà®²à¯ à®•à¯Šà®¨à¯à®¤à®³à®¿à®ªà¯à®ªà¯ à®…à®¤à®¿à®•à®®à®¾à®• à®‰à®³à¯à®³à®¤à®¾à®²à¯ 12 à®•à®Ÿà®²à¯ à®®à¯ˆà®²à¯à®•à®³à¯à®•à¯à®•à¯ à®…à®ªà¯à®ªà®¾à®²à¯ à®šà¯†à®²à¯à®µà®¤à¯ˆà®¤à¯ à®¤à®µà®¿à®°à¯à®•à¯à®•à®µà¯à®®à¯. à®•à®µà®©à®®à®¾à®•à®ªà¯ à®šà¯†à®²à¯à®²à¯à®™à¯à®•à®³à¯! ðŸŒŠ",
        "à®¨à®²à¯à®² à®•à¯‡à®³à¯à®µà®¿! à®šà®®à¯€à®ªà®¤à¯à®¤à®¿à®¯ à®šà®¨à¯à®¤à¯ˆ à®¤à®°à®µà¯à®•à®³à®¿à®©à¯ à®…à®Ÿà®¿à®ªà¯à®ªà®Ÿà¯ˆà®¯à®¿à®²à¯, à®®à¯à®®à¯à®ªà¯ˆà®¯à®¿à®©à¯ à®šà®¾à®šà¯‚à®©à¯ à®Ÿà®¾à®•à¯à®¸à®¿à®²à¯ à®ªà®¾à®®à¯à®ƒà®ªà¯à®°à¯†à®Ÿà¯ â‚¹750-800/à®•à®¿à®²à¯‹à®µà¯à®•à¯à®•à¯à®šà¯ à®šà¯†à®²à¯à®•à®¿à®±à®¤à¯. à®…à®¤à®¿à®• à®¤à¯‡à®µà¯ˆà®¯à¯à®Ÿà®©à¯ à®šà¯à®±à®¾à®®à¯€à®©à¯ (Seer Fish) â‚¹700/à®•à®¿à®²à¯‹à®µà®¿à®²à¯ à®‰à®³à¯à®³à®¤à¯. à®ªà®¾à®®à¯à®ƒà®ªà¯à®°à¯†à®Ÿà¯ à®‡à®©à¯à®±à¯ˆà®¯ à®µà®¿à®²à¯ˆà®¯à®¿à®²à¯ à®µà®¿à®±à¯à®•à®ªà¯ à®ªà®°à®¿à®¨à¯à®¤à¯à®°à¯ˆà®•à¯à®•à®¿à®±à¯‡à®©à¯. à®•à®¾à®©à®¾à®™à¯à®•à¯†à®³à¯à®¤à¯à®¤à®¿ à®µà®¿à®²à¯ˆ â‚¹200/à®•à®¿à®²à¯‹à®µà®¿à®²à¯ à®¨à®¿à®²à¯ˆà®¯à®¾à®• à®‰à®³à¯à®³à®¤à¯. ðŸ’°",
        "à®ªà®´à®®à¯ˆà®µà®¾à®¤ à®ªà®Ÿà®•à¯à®•à®³à¯à®•à¯à®•à¯ à®®à¯€à®©à¯à®ªà®¿à®Ÿà®¿ à®¤à®Ÿà¯ˆà®•à¯à®•à®¾à®²à®®à¯ (à®œà¯‚à®©à¯ 1 - à®œà¯‚à®²à¯ˆ 31) à®ªà¯Šà®°à¯à®¨à¯à®¤à®¾à®¤à¯. à®‡à®¯à®¨à¯à®¤à®¿à®°à®®à®¯à®®à®¾à®•à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿ à®Ÿà®¿à®°à®¾à®²à®°à¯ˆ à®ªà®¯à®©à¯à®ªà®Ÿà¯à®¤à¯à®¤à®¿à®©à®¾à®²à¯, à®‰à®°à®¿à®®à®®à¯ à®¤à®±à¯à®ªà¯‹à®¤à¯ˆà®¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³à®¤à®¾ à®Žà®©à¯à®ªà®¤à¯ˆ à®‰à®±à¯à®¤à®¿à®ªà¯à®ªà®Ÿà¯à®¤à¯à®¤à®µà¯à®®à¯. PM à®®à®¤à¯à®¸à¯à®¯ à®šà®®à¯à®ªà®¤à®¾ à®¯à¯‹à®œà®©à®¾ à®®à®¾à®©à®¿à®¯à®™à¯à®•à®³à¯ˆ à®µà®´à®™à¯à®•à¯à®•à®¿à®±à®¤à¯. ðŸ“‹",
        "à®šà®¿à®±à®¨à¯à®¤ à®¤à®°à®¤à¯à®¤à¯ˆ à®ªà¯†à®±, à®®à¯€à®©à¯à®ªà®¿à®Ÿà®¿à®¤à¯à®¤à®µà¯à®Ÿà®©à¯ à®‰à®Ÿà®©à®Ÿà®¿à®¯à®¾à®• à®ªà®©à®¿à®•à¯à®•à®Ÿà¯à®Ÿà®¿à®¯à®¿à®Ÿà®µà¯à®®à¯. 0-4Â°C à®µà¯†à®ªà¯à®ªà®¨à®¿à®²à¯ˆà®¯à¯ˆ à®ªà®°à®¾à®®à®°à®¿à®•à¯à®•à®µà¯à®®à¯. à®ªà¯†à®°à®¿à®¯ à®®à¯€à®©à¯à®•à®³à¯ˆ 2 à®®à®£à®¿ à®¨à¯‡à®°à®™à¯à®•à®³à¯à®•à¯à®•à¯à®³à¯ à®¤à¯à®£à¯à®Ÿà®¿à®•à¯à®•à®µà¯à®®à¯. ðŸ§Š",
    ]
}

def _get_mock_response(user_input: str, language: str = "en") -> str:
    """Return a contextual mock response based on keywords in the user's message."""
    lower = user_input.lower()
    mock_set = _MOCK_RESPONSES_BY_LANG.get(language, _MOCK_RESPONSES_BY_LANG["en"])

    if any(w in lower for w in ("weather", "wind", "wave", "rain", "storm", "sea condition")):
        return mock_set[1]
    if any(w in lower for w in ("price", "market", "sell", "buy", "rate", "cost")):
        return mock_set[2]
    if any(w in lower for w in ("regulation", "ban", "license", "scheme", "government", "subsidy")):
        return mock_set[3]
    if any(w in lower for w in ("quality", "ice", "fresh", "preserve", "store", "grade")):
        return mock_set[4]

    # Default
    return mock_set[0]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Node: agent
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def agent(state: AgentState) -> Dict[str, Any]:
    """Invoke the LLM with the current message history. Falls back to mock if Bedrock unavailable."""
    lang = state.get("selected_language", "en")
    try:
        llm = _get_llm()
        response = await llm.ainvoke(state["messages"])
    except Exception as e:
        import logging
        logging.warning(f"Bedrock LLM call failed ({type(e).__name__}: {e}), using mock response")
        mock_text = _get_mock_response(state.get("human_input", ""), lang)
        response = AIMessage(content=mock_text)
    return {"messages": state["messages"] + [response]}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Node: tool_executor
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TOOL_MAP = {t.name: t for t in TOOLS}

async def tool_executor(state: AgentState) -> Dict[str, Any]:
    """Execute any tool calls made by the LLM."""
    messages = list(state["messages"])
    last_msg = messages[-1]

    if not isinstance(last_msg, AIMessage) or not last_msg.tool_calls:
        return {"messages": messages}

    tool_outputs = state.get("tool_outputs", [])

    for call in last_msg.tool_calls:
        tool_name = call["name"]
        tool_args = call["args"]

        if tool_name in TOOL_MAP:
            try:
                result = await TOOL_MAP[tool_name].ainvoke(tool_args)
            except Exception as e:
                result = f"âš ï¸ Tool error: {e}"
        else:
            result = f"âš ï¸ Unknown tool: {tool_name}"

        messages.append(ToolMessage(content=str(result), tool_call_id=call["id"]))
        tool_outputs.append({"tool": tool_name, "args": tool_args, "result": str(result)[:500]})

    return {"messages": messages, "tool_outputs": tool_outputs}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Node: memory_update
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def memory_update(state: AgentState) -> Dict[str, Any]:
    """Extract long-term memory from the latest exchange (fire-and-forget)."""
    messages = state["messages"]
    # Find the last AI text response
    ai_response = ""
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content and not msg.tool_calls:
            ai_response = msg.content
            break

    if ai_response:
        try:
            await extract_and_update_long_term_memory(
                user_id=state["user_id"],
                user_message=state["human_input"],
                assistant_response=ai_response,
            )
        except Exception:
            pass  # Don't fail the response if memory extraction fails

    return {}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Routing functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def route_language(state: AgentState) -> Literal["load_context", "end"]:
    """After language_guard: if rejected, go to END; else continue."""
    if state.get("language_accepted"):
        return "load_context"
    return "end"


def route_agent(state: AgentState) -> Literal["tool_executor", "memory_update"]:
    """After agent: if tool calls exist, execute them; else update memory and end."""
    messages = state.get("messages", [])
    if messages:
        last = messages[-1]
        if isinstance(last, AIMessage) and last.tool_calls:
            return "tool_executor"
    return "memory_update"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Build the graph
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_graph() -> StateGraph:
    """Construct and compile the LangGraph agent graph."""
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("language_guard", language_guard)
    workflow.add_node("load_context", load_context)
    workflow.add_node("agent", agent)
    workflow.add_node("tool_executor", tool_executor)
    workflow.add_node("memory_update", memory_update)

    # Entry point
    workflow.set_entry_point("language_guard")

    # Edges
    workflow.add_conditional_edges("language_guard", route_language, {
        "load_context": "load_context",
        "end": END,
    })
    workflow.add_edge("load_context", "agent")
    workflow.add_conditional_edges("agent", route_agent, {
        "tool_executor": "tool_executor",
        "memory_update": "memory_update",
    })
    workflow.add_edge("tool_executor", "agent")      # Loop back after tool execution
    workflow.add_edge("memory_update", END)

    return workflow.compile()


# Singleton graph instance
graph = build_graph()
